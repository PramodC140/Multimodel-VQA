{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3064985,"sourceType":"datasetVersion","datasetId":1876338}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wget tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T16:09:37.564509Z","iopub.execute_input":"2025-01-30T16:09:37.564869Z","iopub.status.idle":"2025-01-30T16:09:43.441247Z","shell.execute_reply.started":"2025-01-30T16:09:37.564841Z","shell.execute_reply":"2025-01-30T16:09:43.440198Z"}},"outputs":[{"name":"stdout","text":"Collecting wget\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\nBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=79fbbdf24a96279d288e75c11b718549c3dc5b385e701ff069a4102ad46051f0\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\nSuccessfully built wget\nInstalling collected packages: wget\nSuccessfully installed wget-3.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel, ViTFeatureExtractor, ViTModel\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nfrom torchvision import transforms\nimport json\nimport os\nimport wget\nimport zipfile\nfrom tqdm import tqdm\nimport os\nimport json\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# Constants\nMAX_LENGTH = 128\nBATCH_SIZE = 32\nLEARNING_RATE = 1e-4\nNUM_EPOCHS = 50\nIMAGE_SIZE = 224\nHIDDEN_SIZE = 768\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T17:30:08.688674Z","iopub.execute_input":"2025-01-30T17:30:08.689004Z","iopub.status.idle":"2025-01-30T17:30:08.694660Z","shell.execute_reply.started":"2025-01-30T17:30:08.688978Z","shell.execute_reply":"2025-01-30T17:30:08.693871Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def prepare_kaggle_daquar(input_dir, output_dir):\n    \"\"\"Prepare DAQUAR dataset from Kaggle format with compound answer handling\"\"\"\n    print(\"Processing Kaggle DAQUAR dataset...\")\n    \n    # Read CSV files\n    train_data = pd.read_csv(os.path.join(input_dir, 'data_train.csv'))\n    eval_data = pd.read_csv(os.path.join(input_dir, 'data_eval.csv'))\n    \n    # Read image lists\n    with open(os.path.join(input_dir, 'train_images_list.txt'), 'r') as f:\n        train_images = [line.strip() for line in f.readlines()]\n    with open(os.path.join(input_dir, 'test_images_list.txt'), 'r') as f:\n        test_images = [line.strip() for line in f.readlines()]\n    \n    # Process answers to handle compound answers\n    def get_all_answers(data):\n        answer_set = set()\n        for answer in data['answer']:\n            # Split compound answers and strip whitespace\n            parts = [part.strip() for part in str(answer).split(',')]\n            answer_set.update(parts)\n        return sorted(list(answer_set))\n    \n    # Create answer vocabulary from both train and eval sets\n    answer_vocab = get_all_answers(pd.concat([train_data, eval_data]))\n    answer_to_idx = {ans: idx for idx, ans in enumerate(answer_vocab)}\n    \n    def create_annotations(data):\n        annotations = []\n        for _, row in data.iterrows():\n            # Split compound answers into individual answers\n            answers = [ans.strip() for ans in str(row['answer']).split(',')]\n            # Use the first answer as the primary answer\n            primary_answer = answers[0]\n            \n            ann = {\n                'image': f\"{row['image_id']}.png\",\n                'question': row['question'],\n                'answer': primary_answer,  # Use only the primary answer\n                'all_answers': answers  # Keep all answers for potential future use\n            }\n            annotations.append(ann)\n        return annotations\n    \n    # Create annotation files\n    train_annotations = create_annotations(train_data)\n    test_annotations = create_annotations(eval_data)\n    \n    # Create processed directory\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Save processed annotations\n    with open(os.path.join(output_dir, 'train_annotations.json'), 'w') as f:\n        json.dump(train_annotations, f)\n    with open(os.path.join(output_dir, 'test_annotations.json'), 'w') as f:\n        json.dump(test_annotations, f)\n    \n    # Save vocabulary\n    with open(os.path.join(output_dir, 'answer_vocab.json'), 'w') as f:\n        json.dump({\n            'answer_vocab': answer_vocab,\n            'answer_to_idx': answer_to_idx\n        }, f)\n    \n    print(f\"Dataset prepared successfully!\")\n    print(f\"Total training examples: {len(train_annotations)}\")\n    print(f\"Total test examples: {len(test_annotations)}\")\n    print(f\"Total unique answers: {len(answer_vocab)}\")\n    \n    # Print first few examples\n    print(\"\\nFirst few training examples:\")\n    for i in range(3):\n        print(f\"Example {i+1}:\")\n        print(f\"Image: {train_annotations[i]['image']}\")\n        print(f\"Question: {train_annotations[i]['question']}\")\n        print(f\"Primary Answer: {train_annotations[i]['answer']}\")\n        print(f\"All Answers: {train_annotations[i]['all_answers']}\\n\")\n    \n    return len(answer_vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T16:34:53.853293Z","iopub.execute_input":"2025-01-30T16:34:53.853622Z","iopub.status.idle":"2025-01-30T16:34:53.864289Z","shell.execute_reply.started":"2025-01-30T16:34:53.853598Z","shell.execute_reply":"2025-01-30T16:34:53.863479Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"class KaggleDAQUARDataset(Dataset):\n    def __init__(self, input_dir, processed_dir, split='train'):\n        self.input_dir = input_dir\n        self.processed_dir = processed_dir\n        self.split = split\n        \n        # Load annotations\n        with open(os.path.join(processed_dir, f'{split}_annotations.json'), 'r') as f:\n            self.annotations = json.load(f)\n            \n        # Load answer vocabulary\n        with open(os.path.join(processed_dir, 'answer_vocab.json'), 'r') as f:\n            vocab_data = json.load(f)\n            self.answer_vocab = vocab_data['answer_vocab']\n            self.answer_to_idx = vocab_data['answer_to_idx']\n            \n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n        \n    def __len__(self):\n        return len(self.annotations)\n        \n    def __getitem__(self, idx):\n        ann = self.annotations[idx]\n        \n        # Load and preprocess image\n        img_path = os.path.join(self.input_dir, 'images', ann['image'])\n        image = Image.open(img_path).convert('RGB')\n        \n        # Process image with ViT feature extractor\n        image_features = self.feature_extractor(images=image, return_tensors=\"pt\")\n        \n        # Tokenize question\n        question_encoding = self.tokenizer(\n            ann['question'],\n            padding='max_length',\n            max_length=MAX_LENGTH,\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        # Get answer index (using primary answer)\n        answer_idx = self.answer_to_idx[ann['answer']]\n        \n        return {\n            'image': image_features.pixel_values[0],\n            'input_ids': question_encoding['input_ids'][0],\n            'attention_mask': question_encoding['attention_mask'][0],\n            'answer': torch.tensor(answer_idx)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T16:35:00.023871Z","iopub.execute_input":"2025-01-30T16:35:00.024180Z","iopub.status.idle":"2025-01-30T16:35:00.031380Z","shell.execute_reply.started":"2025-01-30T16:35:00.024155Z","shell.execute_reply":"2025-01-30T16:35:00.030498Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"    input_dir = '/kaggle/input/processed-daquar-dataset'\n    output_dir = '/kaggle/working/processed'\n    \n    # Prepare dataset\n    print(\"Preparing dataset...\")\n    num_classes = prepare_kaggle_daquar(input_dir, output_dir)\n    \n    print(f\"\\nNumber of answer classes: {num_classes}\")\n    \n    # Test dataset loading\n    print(\"\\nTesting dataset loading...\")\n    try:\n        dataset = KaggleDAQUARDataset(\n            input_dir=input_dir,\n            processed_dir=output_dir,\n            split='train'\n        )\n        print(f\"Successfully loaded dataset with {len(dataset)} examples\")\n        \n        # Test loading first item\n        first_item = dataset[0]\n        print(\"\\nFirst item shapes:\")\n        print(f\"Image: {first_item['image'].shape}\")\n        print(f\"Input IDs: {first_item['input_ids'].shape}\")\n        print(f\"Attention Mask: {first_item['attention_mask'].shape}\")\n        print(f\"Answer: {first_item['answer']}\")\n        \n    except Exception as e:\n        print(f\"Error loading dataset: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T16:35:03.923132Z","iopub.execute_input":"2025-01-30T16:35:03.923473Z","iopub.status.idle":"2025-01-30T16:35:05.117754Z","shell.execute_reply.started":"2025-01-30T16:35:03.923443Z","shell.execute_reply":"2025-01-30T16:35:05.116908Z"}},"outputs":[{"name":"stdout","text":"Preparing dataset...\nProcessing Kaggle DAQUAR dataset...\nDataset prepared successfully!\nTotal training examples: 6795\nTotal test examples: 5673\nTotal unique answers: 582\n\nFirst few training examples:\nExample 1:\nImage: image3.png\nQuestion: what is on the right side of the black telephone and on the left side of the red chair\nPrimary Answer: desk\nAll Answers: ['desk']\n\nExample 2:\nImage: image3.png\nQuestion: what is in front of the white door on the left side of the desk\nPrimary Answer: telephone\nAll Answers: ['telephone']\n\nExample 3:\nImage: image3.png\nQuestion: what is on the desk\nPrimary Answer: book\nAll Answers: ['book', 'scissor', 'papers', 'tape_dispenser']\n\n\nNumber of answer classes: 582\n\nTesting dataset loading...\nSuccessfully loaded dataset with 6795 examples\n\nFirst item shapes:\nImage: torch.Size([3, 224, 224])\nInput IDs: torch.Size([128])\nAttention Mask: torch.Size([128])\nAnswer: 160\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"class CrossModalAttention(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)\n        self.norm1 = nn.LayerNorm(hidden_size)\n        self.norm2 = nn.LayerNorm(hidden_size)\n        self.feed_forward = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size * 4),\n            nn.ReLU(),\n            nn.Linear(hidden_size * 4, hidden_size)\n        )\n        \n    def forward(self, x, y):\n        # Cross attention\n        attended_x, _ = self.attention(x, y, y)\n        x = self.norm1(x + attended_x)\n        \n        # Feed forward\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + ff_output)\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T16:35:08.709651Z","iopub.execute_input":"2025-01-30T16:35:08.710037Z","iopub.status.idle":"2025-01-30T16:35:08.718393Z","shell.execute_reply.started":"2025-01-30T16:35:08.710008Z","shell.execute_reply":"2025-01-30T16:35:08.717453Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"class KaggleDAQUARDataset(Dataset):\n    def __init__(self, input_dir, processed_dir, split='train', transform=None):\n        self.input_dir = input_dir\n        self.processed_dir = processed_dir\n        self.split = split\n        \n        # Load annotations\n        with open(os.path.join(processed_dir, f'{split}_annotations.json'), 'r') as f:\n            self.annotations = json.load(f)\n            \n        # Load answer vocabulary\n        with open(os.path.join(processed_dir, 'answer_vocab.json'), 'r') as f:\n            vocab_data = json.load(f)\n            self.answer_vocab = vocab_data['answer_vocab']\n            self.answer_to_idx = vocab_data['answer_to_idx']\n            \n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n        \n    def __len__(self):\n        return len(self.annotations)\n        \n    def __getitem__(self, idx):\n        ann = self.annotations[idx]\n        \n        # Load and preprocess image\n        img_path = os.path.join(self.input_dir, 'images', ann['image'])\n        image = Image.open(img_path).convert('RGB')\n        \n        # Process image with ViT feature extractor (handles resizing and normalization)\n        image_features = self.feature_extractor(images=image, return_tensors=\"pt\")\n        \n        # Tokenize question\n        question_encoding = self.tokenizer(\n            ann['question'],\n            padding='max_length',\n            max_length=MAX_LENGTH,\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        # Convert answer to index\n        answer_idx = self.answer_to_idx[ann['answer']]\n        \n        return {\n            'image': image_features.pixel_values[0],  # Remove the batch dimension\n            'input_ids': question_encoding['input_ids'][0],\n            'attention_mask': question_encoding['attention_mask'][0],\n            'answer': torch.tensor(answer_idx)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T16:35:10.387131Z","iopub.execute_input":"2025-01-30T16:35:10.387476Z","iopub.status.idle":"2025-01-30T16:35:10.394830Z","shell.execute_reply.started":"2025-01-30T16:35:10.387448Z","shell.execute_reply":"2025-01-30T16:35:10.393984Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"class MultimodalVQAModel(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        \n        # Load pre-trained models\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n        \n        # Cross-modal attention modules\n        self.image_to_text_attention = CrossModalAttention(HIDDEN_SIZE)\n        self.text_to_image_attention = CrossModalAttention(HIDDEN_SIZE)\n        \n        # Freeze some layers for transfer learning\n        for param in self.bert.parameters():\n            param.requires_grad = False\n        for param in self.vit.parameters():\n            param.requires_grad = False\n            \n        # Unfreeze the last few layers\n        for param in self.bert.encoder.layer[-2:].parameters():\n            param.requires_grad = True\n        for param in self.vit.encoder.layer[-2:].parameters():\n            param.requires_grad = True\n        \n        # Final fusion and classification layers\n        self.fusion = nn.Sequential(\n            nn.Linear(HIDDEN_SIZE * 2, HIDDEN_SIZE),\n            nn.ReLU(),\n            nn.Dropout(0.5)\n        )\n        \n        self.classifier = nn.Linear(HIDDEN_SIZE, num_classes)\n        \n    def forward(self, image, input_ids, attention_mask):\n        # Process image with ViT\n        image_features = self.vit(image).last_hidden_state  # [batch_size, num_patches, hidden_size]\n        \n        # Process text with BERT\n        text_features = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        ).last_hidden_state  # [batch_size, seq_len, hidden_size]\n        \n        # Cross-modal attention\n        attended_image = self.image_to_text_attention(image_features, text_features)\n        attended_text = self.text_to_image_attention(text_features, image_features)\n        \n        # Pool attended features\n        image_pooled = attended_image.mean(dim=1)  # [batch_size, hidden_size]\n        text_pooled = attended_text.mean(dim=1)    # [batch_size, hidden_size]\n        \n        # Concatenate pooled features\n        combined_features = torch.cat((image_pooled, text_pooled), dim=1)\n        \n        # Final fusion and classification\n        fused_features = self.fusion(combined_features)\n        logits = self.classifier(fused_features)\n        \n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T16:35:12.110762Z","iopub.execute_input":"2025-01-30T16:35:12.111084Z","iopub.status.idle":"2025-01-30T16:35:12.118403Z","shell.execute_reply.started":"2025-01-30T16:35:12.111059Z","shell.execute_reply":"2025-01-30T16:35:12.117488Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def create_data_loaders(input_dir, processed_dir, batch_size):\n    # Create datasets without additional transforms since ViT feature extractor handles preprocessing\n    train_dataset = KaggleDAQUARDataset(\n        input_dir=input_dir,\n        processed_dir=processed_dir,\n        split='train'\n    )\n    \n    val_dataset = KaggleDAQUARDataset(\n        input_dir=input_dir,\n        processed_dir=processed_dir,\n        split='test'  # Using test split for validation\n    )\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True\n    )\n    \n    return train_loader, val_loader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T16:35:14.654878Z","iopub.execute_input":"2025-01-30T16:35:14.655160Z","iopub.status.idle":"2025-01-30T16:35:14.659899Z","shell.execute_reply.started":"2025-01-30T16:35:14.655140Z","shell.execute_reply":"2025-01-30T16:35:14.659044Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"def train_model(input_dir, processed_dir, num_classes):\n    # Create data loaders\n    train_loader, val_loader = create_data_loaders(input_dir, processed_dir, BATCH_SIZE)\n    \n    # Initialize model\n    print(\"Initializing model...\")\n    model = MultimodalVQAModel(num_classes=num_classes)\n    \n    # Loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n    \n    # Train the model\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Training on {device}\")\n    \n    model = model.to(device)\n    \n    for epoch in range(NUM_EPOCHS):\n        model.train()\n        train_loss = 0\n        correct = 0\n        total = 0\n        \n        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')\n        for batch in progress_bar:\n            image = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            answers = batch['answer'].to(device)\n            \n            optimizer.zero_grad()\n            \n            outputs = model(image, input_ids, attention_mask)\n            loss = criterion(outputs, answers)\n            \n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += answers.size(0)\n            correct += predicted.eq(answers).sum().item()\n            \n            progress_bar.set_postfix({\n                'loss': f'{train_loss/total:.4f}',\n                'acc': f'{100.*correct/total:.2f}%'\n            })\n        \n        # Validation\n        model.eval()\n        val_loss = 0\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc='Validation'):\n                image = batch['image'].to(device)\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                answers = batch['answer'].to(device)\n                \n                outputs = model(image, input_ids, attention_mask)\n                loss = criterion(outputs, answers)\n                \n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                val_total += answers.size(0)\n                val_correct += predicted.eq(answers).sum().item()\n        \n        print(f'\\nEpoch {epoch+1}/{NUM_EPOCHS}:')\n        print(f'Train Loss: {train_loss/len(train_loader):.4f}, Accuracy: {100.*correct/total:.2f}%')\n        print(f'Val Loss: {val_loss/len(val_loader):.4f}, Accuracy: {100.*val_correct/val_total:.2f}%')\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T16:35:17.516161Z","iopub.execute_input":"2025-01-30T16:35:17.516500Z","iopub.status.idle":"2025-01-30T16:35:17.526768Z","shell.execute_reply.started":"2025-01-30T16:35:17.516472Z","shell.execute_reply":"2025-01-30T16:35:17.525985Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"    input_dir = '/kaggle/input/processed-daquar-dataset'\n    processed_dir = '/kaggle/working/processed'\n    \n    # Prepare dataset\n    print(\"Preparing dataset...\")\n    num_classes = prepare_kaggle_daquar(input_dir, processed_dir)\n    \n    print(f\"\\nNumber of answer classes: {num_classes}\")\n    \n    # Train the model\n    model = train_model(input_dir, processed_dir, num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T16:35:19.807086Z","iopub.execute_input":"2025-01-30T16:35:19.807366Z","iopub.status.idle":"2025-01-30T17:29:50.116994Z","shell.execute_reply.started":"2025-01-30T16:35:19.807345Z","shell.execute_reply":"2025-01-30T17:29:50.116067Z"}},"outputs":[{"name":"stdout","text":"Preparing dataset...\nProcessing Kaggle DAQUAR dataset...\nDataset prepared successfully!\nTotal training examples: 6795\nTotal test examples: 5673\nTotal unique answers: 582\n\nFirst few training examples:\nExample 1:\nImage: image3.png\nQuestion: what is on the right side of the black telephone and on the left side of the red chair\nPrimary Answer: desk\nAll Answers: ['desk']\n\nExample 2:\nImage: image3.png\nQuestion: what is in front of the white door on the left side of the desk\nPrimary Answer: telephone\nAll Answers: ['telephone']\n\nExample 3:\nImage: image3.png\nQuestion: what is on the desk\nPrimary Answer: book\nAll Answers: ['book', 'scissor', 'papers', 'tape_dispenser']\n\n\nNumber of answer classes: 582\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Initializing model...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Training on cuda\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 213/213 [03:28<00:00,  1.02it/s, loss=0.1685, acc=6.56%]\nValidation: 100%|██████████| 178/178 [01:59<00:00,  1.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10:\nTrain Loss: 5.3746, Accuracy: 6.56%\nVal Loss: 4.4467, Accuracy: 14.26%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 213/213 [03:27<00:00,  1.03it/s, loss=0.1345, acc=14.36%]\nValidation: 100%|██████████| 178/178 [01:59<00:00,  1.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/10:\nTrain Loss: 4.2903, Accuracy: 14.36%\nVal Loss: 3.9223, Accuracy: 21.82%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 213/213 [03:27<00:00,  1.03it/s, loss=0.1211, acc=19.25%]\nValidation: 100%|██████████| 178/178 [01:59<00:00,  1.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/10:\nTrain Loss: 3.8631, Accuracy: 19.25%\nVal Loss: 3.7164, Accuracy: 23.30%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 213/213 [03:27<00:00,  1.03it/s, loss=0.1127, acc=22.68%]\nValidation: 100%|██████████| 178/178 [01:58<00:00,  1.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/10:\nTrain Loss: 3.5937, Accuracy: 22.68%\nVal Loss: 3.5973, Accuracy: 24.29%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 213/213 [03:28<00:00,  1.02it/s, loss=0.1061, acc=25.61%]\nValidation: 100%|██████████| 178/178 [01:58<00:00,  1.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/10:\nTrain Loss: 3.3854, Accuracy: 25.61%\nVal Loss: 3.5263, Accuracy: 25.47%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 213/213 [03:27<00:00,  1.03it/s, loss=0.1005, acc=27.79%]\nValidation: 100%|██████████| 178/178 [01:58<00:00,  1.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/10:\nTrain Loss: 3.2071, Accuracy: 27.79%\nVal Loss: 3.4622, Accuracy: 25.89%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 213/213 [03:27<00:00,  1.02it/s, loss=0.0950, acc=30.95%]\nValidation: 100%|██████████| 178/178 [01:58<00:00,  1.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/10:\nTrain Loss: 3.0298, Accuracy: 30.95%\nVal Loss: 3.4073, Accuracy: 26.79%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 213/213 [03:27<00:00,  1.02it/s, loss=0.0907, acc=33.51%]\nValidation: 100%|██████████| 178/178 [01:59<00:00,  1.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/10:\nTrain Loss: 2.8936, Accuracy: 33.51%\nVal Loss: 3.3818, Accuracy: 27.32%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 213/213 [03:28<00:00,  1.02it/s, loss=0.0865, acc=35.58%]\nValidation: 100%|██████████| 178/178 [01:58<00:00,  1.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/10:\nTrain Loss: 2.7584, Accuracy: 35.58%\nVal Loss: 3.3322, Accuracy: 28.03%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 213/213 [03:28<00:00,  1.02it/s, loss=0.0825, acc=37.26%]\nValidation: 100%|██████████| 178/178 [01:59<00:00,  1.50it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/10:\nTrain Loss: 2.6333, Accuracy: 37.26%\nVal Loss: 3.3250, Accuracy: 27.99%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}